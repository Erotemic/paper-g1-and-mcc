\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

%% extra
\usepackage{listings}
\usepackage{amsmath} 
\usepackage{xcolor}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}
\lstset{style=mystyle}


\definecolor{kwgreen}{HTML}{3EAE2B}
\definecolor{kwblue}{HTML}{0068C7}
\definecolor{kworange}{HTML}{EF7724}
\definecolor{kwred}{HTML}{F42836}
\definecolor{kwdarkgreen}{HTML}{2E5524}
\definecolor{kwdarkblue}{HTML}{003765}


\newcommand{\TP}[1]{\textcolor{kwgreen}{\texttt{\textbf{TP}}}}
\newcommand{\FP}[1]{\textcolor{kwred}{\texttt{\textbf{FP}}}}
\newcommand{\TN}[1]{\textcolor{kwblue}{\texttt{\textbf{TN}}}}
\newcommand{\FN}[1]{\textcolor{kworange}{\texttt{\textbf{FN}}}}

\newcommand{\PPV}[1]{\texttt{\textbf{PPV}}}
\newcommand{\TPR}[1]{\texttt{\textbf{TPR}}}
\newcommand{\MCC}[1]{\texttt{\textbf{MCC}}}
\newcommand{\Gm}[0]{\texttt{\textbf{G1}}}
\newcommand{\Fm}[0]{\texttt{\textbf{F1}}}



% Based on my blog post
% https://erotemic.wordpress.com/2019/10/23/closed-form-of-the-mcc-when-tn-inf/

\title{The G1-measure is the limit of the MCC as the number of True negatives approaches infinity}

\author{Jon Crall\thanks{https://github.com/Erotemic/ \texttt{erotemic@gmail.com}} \\
	Staff Researcher and Engineer\\
	Kitware Inc.\\
	\texttt{jon.crall@kitware.com} \\
	 \\
}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Jon Crall},
pdfkeywords={Confusion Matrix, Binary Classification, G1, F1, MCC},
}

\begin{document}
\maketitle

\begin{abstract}
    The performance of a binary classifier is described by a confusion matrix.
    The MCC F1, and G1 scores are scalars that summarize a confusion matrix in
    a single number. Both the F1 and G1 scores are based on
    precision and recall (the harmonic mean, and geometric mean respectively).
    Precision and recall are based on 3 / 4 entries in a confusion matrix: \TP{},
    \FN{}, and \FP{}. In contrast, the Matthew's Correlation Coefficient (MCC) takes
    into account \TN{} and thus measures all four entries of the confusion matrix.

    In this short paper I prove that the G1-measure is the limit of the MCC
    as the number of true negatives approaches infinity.
\end{abstract}


% keywords can be removed
\keywords{Confusion Matrix \and Binary Classification \and G1 \and F1 \and MCC}


\section{Introduction}

Evaluation of binary classifiers is central to the scientific analysis of machine learning models \cite{powers_evaluation_2011}.

Given a finite set of examples, a machine learning algorithm that predicts a
binary label for each example, and a set of true labels for each example, the
quality of the machine learning model can be expressed using a $2 \times 2$
confusion matrix.

\begin{equation}
\begin{bmatrix}
    \TP{} & \FP{} \\
    \FN{} & \TN{} 
\end{bmatrix}	
\end{equation}

A binary confusion matrix counts the number of true positives (\TP{}), true negatives (\TN{}), false positives (\FP{}), and false negatives (\FN{}).

\section{The Relationship Between MCC and G1}
\label{sec:headings}

The Fowlkes--Mallows index \cite{fowlkes_method_1983} is defined as:

\begin{equation}
    \Gm{} = \sqrt{\PPV{} \cdot \TPR{}} = \sqrt{\frac{\TP{}}{\TP{} + \FP{}} \cdot \frac{\TP{}}{\TP{} + \FN{}}}
\end{equation}

The Matthews Correlation Coefficient \cite{matthews_comparison_1975} is defined as:

\begin{equation}
    \MCC{} = \frac{
        \TP{} \cdot \TN{} - \FP{} \cdot \FN{}
    }
    {\sqrt{
        (\TP{} + \FP{}) (\TP{} + \FN{}) (\TN{} + \FP{}) (\TN{} + \FN{})
    }}
\end{equation}

Consider the limit of the MCC as the number of true negatives approaches infinity.

\begin{equation}
    \lim_{\TN{} \to \infty}
    \frac{
        \TP{} \cdot \TN{} - \FP{} \cdot \FN{}
    }
    {\sqrt{
        (\TP{} + \FP{}) (\TP{} + \FN{}) (\TN{} + \FP{}) (\TN{} + \FN{})
    }}
\end{equation}


In the limit the terms containing 

This simplifies to:

\begin{equation}
    \lim_{\TN{} \to \infty} \MCC{} = 
    \frac{\TP{}}
    {\sqrt{
        (\TP{} + \FP{}) \cdot (\TP{} + \FN{}) 
    }}
\end{equation}

The correctness of this simplification can be verified using SymPy \cite{sympy17}.

\begin{lstlisting}[language=Python]

from sympy import sqrt, symbols, simplify
from sympy.series import limit

tp, tn, fp, fn = symbols(
    ['tp', 'tn', 'fp', 'fn'], 
    integer=True,
    negative=False)

numer = (tp * tn - fp * fn)
denom = sqrt(
    (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
mcc = numer / denom

mcc_lim = limit(mcc, tn, float('inf'))

# We claim the above is equal to
mcc_claim = tp / sqrt((tp + fn) * ((tp + fp)))

# Check the claim
assert simplify(mcc_lim - mcc_claim) == 0

\end{lstlisting}

Now rearranging the equation for G1, we find it is equivalent to the limit of the MCC as the number of true negatives approaches infinity.

\begin{align*}
    \Gm{} &= \sqrt{\frac{\TP{}}{\TP{} + \FP{}} \cdot \frac{\TP{}}{\TP{} + \FN{}}} \\
          &= \sqrt{\frac{\TP{}^2}{(\TP{} + \FP{}) \cdot (\TP{} + \FN{})}} \\
          &= \frac{\TP{}}{\sqrt{(\TP{} + \FP{}) \cdot (\TP{} + \FN{})}} \\
          &= \lim_{\TN{} \to \infty} \MCC{}
\end{align}
%\end{equation}

Again, we can verify there are no mistakes using SymPy.

\begin{lstlisting}[language=Python]

from sympy import sqrt, symbols, simplify

tp, fp, fn = symbols(
    ['tp', 'fp', 'fn'], 
    integer=True,
    negative=False)

# Our definition of G1
G1_def = sqrt((tp / (tp + fn)) * (tp / (tp + fp)))

# We claim that it is equal to this simplification
G1_claim = tp / sqrt((tp + fn) * ((tp + fp)))

# Check the claim
assert simplify(G1_def - G1_claim) == 0

\end{lstlisting}

And that's a good place to stop \square{}.


\section{Conclusion}

This paper proves that the limit of the MCC as the number of true negatives
goes to infinity is equivalent to the G1 score / Fowlkes--Mallows index /
geometric mean of precision and recall.

This is a useful insight in open world problems where the number of true
negative cases grows faster than the number of other confusion categories.




\end{document}
